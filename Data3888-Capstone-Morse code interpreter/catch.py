# -*- coding: utf-8 -*-
"""catch.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CFO978bBf5OIWn0wuLc6MEByaQTJvF0L
"""

from google.colab import drive
drive.mount('/content/drive')

# !pip install catch22

# pip install -U scikit-learn

from scipy.io import wavfile
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.model_selection import train_test_split

"""# Orginal chatch fuction"""

def streaming_classifier_with_input (samplerate,Y):

    xtime = np.array(range(0, len(Y)))/samplerate
    window_size = samplerate
    increment = int(window_size/3)
    thresh = 1500


    predicted_labels = []  # stores predicted
    lower_interval = 0  # used to increment window
    max_time = int(max(xtime) * samplerate)

    predicted = []
    # initialing signal vector
    counter = 0
    is_event = []


    while (max_time > lower_interval + window_size):

        if max_time < lower_interval + window_size + increment:
            upper_interval = max_time
        else:
            upper_interval = lower_interval + window_size

        interval = Y[lower_interval:upper_interval]
        xinterval = xtime[lower_interval:upper_interval]  # gets corresponding time

        zerocrossing = (np.diff(np.sign(interval)) != 0).sum()
        Mean_value = np.mean(interval)
        standarddeviation = round(np.std(interval),3)
        #print(zerocrossing,Mean_value,standarddeviation,counter,lower_interval,upper_interval)

        # If it is a event, recored it as True and add one to counter
        if standarddeviation > thresh and upper_interval != max_time:
            is_event.append(True)
            counter = counter + 1
            lower_interval = lower_interval + increment

        # If ends, and the counter is greater than 0 which means it has event not finished
        elif upper_interval == max_time and counter > 0:
            begin_time = lower_interval - increment * counter
            end_time = max_time
            predicted.append([begin_time,end_time,end_time-begin_time,Y[begin_time:end_time]])
            #print(begin_time,end_time)
            lower_interval = lower_interval + increment

        # If it is not a event, back to its previous one and adjust whether its previous is event or not
        else:
            is_event.append(False)

            if len(is_event) == 1: #first event
                lower_interval = lower_interval + increment
            elif is_event[-2] == True: #if there was an event since this is after the event has ended
                begin_time = lower_interval - increment * counter
                end_time = lower_interval - increment + window_size

                predicted.append([begin_time,end_time,end_time-begin_time,Y[begin_time:end_time]])
                #print(begin_time,end_time,end_time-begin_time)
                lower_interval = end_time
            else:
                lower_interval = lower_interval + increment
            counter = 0

        
    df = pd.DataFrame(predicted,columns=['begin','end','Long','Values'])
    return df
    #return predicted,eventtime

"""## Nosiy of AbsSum"""

def streaming_nosiy (samplerate,Y):

    xtime = np.array(range(0, len(Y)))/samplerate
    window_size = samplerate
    increment = int(window_size/3)
    thresh = 1500


    predicted_labels = []  # stores predicted
    lower_interval = 0  # used to increment window
    max_time = int(max(xtime) * samplerate)

    abssumlist = []
    # initialing signal vector

    while (max_time > lower_interval + window_size):

        if max_time < lower_interval + window_size + increment:
            upper_interval = max_time
        else:
            upper_interval = lower_interval + window_size

        interval = Y[lower_interval:upper_interval]
        xinterval = xtime[lower_interval:upper_interval]  # gets corresponding time

        zerocrossing = (np.diff(np.sign(interval)) != 0).sum()
        Mean_value = np.mean(interval)
        standarddeviation = round(np.std(interval),3)
        abssum = sum(map(abs, interval))/10000 #adds up to see amount of signal per second 
        #print(abssum,standarddeviation,counter,lower_interval,upper_interval)
        abssumlist.append(abssum)
       
        lower_interval = lower_interval + increment

    return abssumlist
    #return predicted,eventtime

pathlist_noisy = ["/content/drive/MyDrive/Data3888/noise/noise_jack_top_v1.wav",
        "/content/drive/MyDrive/Data3888/noise/noise_jack_top_v2_dodgy.wav",
        "/content/drive/MyDrive/Data3888/noise/noise_jack_top_v3.wav",
        "/content/drive/MyDrive/Data3888/noise/noise_jack_top_v4.wav",
        "/content/drive/MyDrive/Data3888/noise/noise_olli_top_v1_badafter25.wav",
        "/content/drive/MyDrive/Data3888/noise/noise_olli_top_v2.wav",
        "/content/drive/MyDrive/Data3888/noise/noise_olli_top_v3_talking.wav",
        "/content/drive/MyDrive/Data3888/noise/noise_olli_top_v4_talking.wav"]
  

nosisylist = []
for i in pathlist_noisy:
  samplerate, Y = wavfile.read(i)
  nosisylist = nosisylist + streaming_nosiy(samplerate,Y)
print(max(nosisylist),min(nosisylist),np.mean(nosisylist),np.median(nosisylist))

nosisylist.sort()
I = int(0.999*len(nosisylist))
print(np.mean(nosisylist),np.median(nosisylist),nosisylist[I])
plt.hist(nosisylist)
plt.show()

"""# AbsSum tset"""

def streaming_classifier_Noraml(samplerate,Y):

    xtime = np.array(range(0, len(Y)))/samplerate
    window_size = samplerate
    increment = int(window_size/3)
    thresh = 750


    predicted_labels = []  # stores predicted
    lower_interval = 0  # used to increment window
    max_time = int(max(xtime) * samplerate)

    predicted = []
    # initialing signal vector
    counter = 0
    is_event = []


    while (max_time > lower_interval + window_size):

        if max_time < lower_interval + window_size + increment:
            upper_interval = max_time
        else:
            upper_interval = lower_interval + window_size

        interval = Y[lower_interval:upper_interval]
        xinterval = xtime[lower_interval:upper_interval]  # gets corresponding time

        zerocrossing = (np.diff(np.sign(interval)) != 0).sum()
        Mean_value = np.mean(interval)
        standarddeviation = round(np.std(interval),3)
        abssum = sum(map(abs, interval))/10000
        #print(abssum,standarddeviation,counter,lower_interval,upper_interval)

        # If it is a event, recored it as True and add one to counter
        if abssum > thresh and upper_interval != max_time:
            is_event.append(True)
            counter = counter + 1
            lower_interval = lower_interval + increment

        # If ends, and the counter is greater than 0 which means it has event not finished
        elif upper_interval == max_time and counter > 0:
            begin_time = lower_interval - increment * counter
            end_time = max_time
            predicted.append([begin_time,end_time,end_time-begin_time,Y[begin_time:end_time]])
            #print(begin_time,end_time)
            lower_interval = lower_interval + increment

        # If it is not a event, back to its previous one and adjust whether its previous is event or not
        else:
            is_event.append(False)

            if len(is_event) == 1:
                lower_interval = lower_interval + increment
            elif is_event[-2] == True:
                begin_time = lower_interval - increment * counter
                end_time = lower_interval - increment + window_size

                predicted.append([begin_time,end_time,end_time-begin_time,Y[begin_time:end_time]])
                #print(begin_time,end_time,end_time-begin_time)
                lower_interval = end_time
            else:
                lower_interval = lower_interval + increment
            counter = 0

        
    df = pd.DataFrame(predicted,columns=['begin','end','Long','Values'])
    return df
    #return predicted,eventtime

"""## Noraml Blink test"""

busamplerate, Y = wavfile.read("/content/drive/MyDrive/Data3888/blinking/blink_olli_bot_v1.wav")
streaming_classifier_Noraml(samplerate,Y)

pathlist_1 = ["/content/drive/MyDrive/Data3888/blinking/blink_olli_bot_v1.wav",
        "/content/drive/MyDrive/Data3888/blinking/blinking_jack_top_v1.wav",
        "/content/drive/MyDrive/Data3888/blinking/blinking_jack_top_v2.wav",
        "/content/drive/MyDrive/Data3888/blinking/blinking_ollie_top_v1.wav"]
Normal_blink=[]
for i in pathlist_1:
  samplerate, Y = wavfile.read(i)
  result = streaming_classifier_Noraml(samplerate,Y)
  Normal_blink.append(result)
Normal_blink = pd.concat(Normal_blink)
Normal_blink['Type'] = "Normal"

for i in pathlist_1:
  samplerate, Y = wavfile.read(i)
  xtime = np.array(range(0, len(Y)))/samplerate
  plt.figure(figsize=(20,5))
  plt.plot(xtime, Y)
  result = streaming_classifier_Noraml(samplerate,Y)
  for i in range(0,len(result)):
    begin = int(result.iloc[i].at['begin'])
    end = int(result.iloc[i].at['end'])
    Y = result.iloc[i].at['Values']

    xtime = np.array(range(begin, begin+len(Y)))/samplerate
    plt.plot(xtime, Y, color='red')

"""## Long Blink test"""

pathlist_2 = ["/content/drive/MyDrive/Data3888/blinking/longblinkinh_ollie_bot_v2.wav",
        "/content/drive/MyDrive/Data3888/blinking/longblink_olli_top_v1.wav",
        "/content/drive/MyDrive/Data3888/blinking/longblink_jack_top_v1.wav"]
Long_blink=[]
for i in pathlist_2:
  samplerate, Y = wavfile.read(i)
  result = streaming_classifier_Noraml(samplerate,Y)
  Long_blink.append(result)
Long_blink = pd.concat(Long_blink)
Long_blink['Type'] = "Long"

for i in pathlist_2:
  samplerate, Y = wavfile.read(i)
  xtime = np.array(range(0, len(Y)))/samplerate
  plt.figure(figsize=(20,5))
  plt.plot(xtime, Y)
  result = streaming_classifier_Noraml(samplerate,Y)
  for i in range(0,len(result)):
    begin = int(result.iloc[i].at['begin'])
    end = int(result.iloc[i].at['end'])
    Y = result.iloc[i].at['Values']

    xtime = np.array(range(begin, begin+len(Y)))/samplerate
    plt.plot(xtime, Y, color='red')

"""## Double test"""

pathlist_3 = ["/content/drive/MyDrive/Data3888/double_blink/doubleblink_jack_top_v1.wav",
        "/content/drive/MyDrive/Data3888/double_blink/doublelink_jack_bot_v1.wav",
        "/content/drive/MyDrive/Data3888/double_blink/doublelink_jack_bot_v2.wav",
        "/content/drive/MyDrive/Data3888/double_blink/doublelink_jack_top_v2.wav"]
Double_blink=[]
for i in pathlist_3:
  samplerate, Y = wavfile.read(i)
  result = streaming_classifier_Noraml(samplerate,Y)
  Double_blink.append(result)
Double_blink = pd.concat(Double_blink)
Double_blink['Type'] = "Double"

for i in pathlist_3:
  samplerate, Y = wavfile.read(i)
  xtime = np.array(range(0, len(Y)))/samplerate
  plt.figure(figsize=(20,7))
  plt.plot(xtime, Y)
  result = streaming_classifier_Noraml(samplerate,Y)
  for i in range(0,len(result)):
    begin = int(result.iloc[i].at['begin'])
    end = int(result.iloc[i].at['end'])
    Y = result.iloc[i].at['Values']

    xtime = np.array(range(begin, begin+len(Y)))/samplerate
    plt.plot(xtime, Y, color='red')

"""# Combine table and Features

## Catch22
"""

EventFrame = pd.concat([Normal_blink,Long_blink,Double_blink])
EventFrame

"""## Peak and Withs

checking how to code the peaks and long functions
"""

import matplotlib.pyplot as plt
from scipy.signal import find_peaks

peakchecking=[]
for i in EventFrame['Values']:
  peaks, properties = find_peaks(abs(i),height=1700, width=200)
  print(peaks, properties["widths"])
  print(sum(np.diff(peaks)>400)+1)
  peakchecking.append(sum(np.diff(peaks)>400)+1)
  if (peaks[-1]-peaks[0])>7000:
    print("long!")
  plt.plot(i)
  plt.plot(peaks,i[peaks],"x")
  plt.hlines(y=properties["width_heights"], xmin=properties["left_ips"], xmax=properties["right_ips"], color="C1")
  plt.show()

plt.plot(peakchecking)
plt.show()
plt.hist(peakchecking)
plt.show()

import matplotlib.pyplot as plt
from scipy.signal import find_peaks

def peaks(y): #gives the number of peaks
  y=np.array(y)
  peaks, properties = find_peaks(abs(y),height=1700, width=200)
  return sum(np.diff(peaks)>400)+1

def betweenlastpeaks(y): #gives diff in indexes of first and last peak
  y=np.array(y)
  peaks, properties = find_peaks(abs(y),height=1700, width=200)
  return peaks[-1]-peaks[0]

col = [
    'Type',
    'Peaks',
    'Len_between_peaks',
    'DN_HistogramMode_5',
    'DN_HistogramMode_10',
    'CO_f1ecac',
    'CO_FirstMin_ac',
    'CO_HistogramAMI_even_2_5',
    'CO_trev_1_num',
    'MD_hrv_classic_pnn40',
    'SB_BinaryStats_mean_longstretch1',
    'SB_TransitionMatrix_3ac_sumdiagcov',
    'PD_PeriodicityWang_th0_01',
    'CO_Embed2_Dist_tau_d_expfit_meandiff',
    'IN_AutoMutualInfoStats_40_gaussian_fmmi',
    'FC_LocalSimple_mean1_tauresrat',
    'DN_OutlierInclude_p_001_mdrmd',
    'DN_OutlierInclude_n_001_mdrmd',
    'SP_Summaries_welch_rect_area_5_1',
    'SB_BinaryStats_diff_longstretch0',
    'SB_MotifThree_quantile_hh',
    'SC_FluctAnal_2_rsrangefit_50_1_logi_prop_r1',
    'SC_FluctAnal_2_dfa_50_1_2_logi_prop_r1',
    'SP_Summaries_welch_rect_centroid',
    'FC_LocalSimple_mean3_stderr']
df=pd.DataFrame(columns= col)

from catch22 import catch22_all

for i in range(0,len(EventFrame)):
  current_row = EventFrame[i:i+1]
  current_type = current_row['Type'].to_string().split()[1]
  Y = sum(current_row["Values"]).tolist()
  t = catch22_all(Y)
  features = t["values"]
  features.insert(0,current_type)
  features.insert(1,peaks(Y))
  features.insert(1,betweenlastpeaks(Y))
  df.loc[i]=features
df

"""#Feature Screening

## Anova test
"""

df_NandD=pd.concat([df.groupby(['Type']).apply(lambda x: list(x[c])) for c in col[1:]], axis = 1)
df_NandD.columns = col[1:]
df_NandD.reset_index(inplace=True)

df_NandD

import scipy.stats as stats
from scipy.stats import f_oneway
selectfeature = []
selectvalue = []
for i in range(1,len(col)):
  Groups = df_NandD.iloc[:,i]
  D = Groups.iloc[0]
  L = Groups.iloc[1]
  N = Groups.iloc[2]
  stat, p = f_oneway(D,L,N)
  if p < 0.01:
    selectfeature.append(col[i])
    selectvalue.append(p)

selectfeature = dict(zip(selectfeature,selectvalue))

col_AN = list(dict(sorted(selectfeature.items(), key=lambda item: item[1])).keys())
col_AN_10 = col_2[0:10]
col_AN_5 = col_2[0:5]
col_4 = col[1:]

"""## RFE"""

from sklearn.svm import SVC
from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression

label = df['Type']
feature = df[col_4]

def LR_RFE(n):
    LR = LogisticRegression(solver='liblinear')
    rfe = RFE(estimator=LR, n_features_to_select=n, step=5)
    rfe.fit(feature, label)
    return rfe.support_

selectlist = LR_RFE(5)
col_RFE_5 = [col_4[i] for i in [i for i, x in enumerate(selectlist) if x]]
selectlist = LR_RFE(5)
col_RFE_10 = [col_4[i] for i in [i for i, x in enumerate(selectlist) if x]]

"""##Extra-tree"""

from sklearn.ensemble import ExtraTreesClassifier

def Extra_tree():
    Extra_model = ExtraTreesClassifier(random_state = 100)
    Extra_model.fit(feature, label)
    return Extra_model.feature_importances_

ET = Extra_tree()
ET

ETfeature = dict(zip(df[col_4],ET))
ET_sort = list(dict(sorted(ETfeature.items(), key=lambda item: item[1])).keys())
ET_sort.reverse()
col_ET_5 = ET_sort[0:5]
col_ET_10 = ET_sort[0:10]

"""# Modles Selection"""

from sklearn.model_selection import train_test_split
#from sklearn.datasets import make_classification
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
x1 = df[col_4]
x_col_AN = df[col_AN]
x_col_AN_5 = df[col_AN_5]
x_RFE_5 = df[col_RFE_5]
x_RFE_10 = df[col_RFE_10]
x_ET_5 = df[col_ET_5]
x_ET_10 = df[col_ET_10]
y = df['Type']

"""## Logistic regression"""

logreg = LogisticRegression(solver='liblinear')
scores1 = cross_val_score(logreg, x1, y, scoring='accuracy', cv=50)
scores2 = cross_val_score(logreg, x_col_AN, y, scoring='accuracy', cv=50)
scores3 = cross_val_score(logreg, x_col_AN_5, y, scoring='accuracy', cv=50)
scores4 = cross_val_score(logreg, x_RFE_5, y, scoring='accuracy', cv=50)
scores5 = cross_val_score(logreg, x_RFE_10, y, scoring='accuracy', cv=50)
scores6 = cross_val_score(logreg, x_ET_5, y, scoring='accuracy', cv=50)
scores7 = cross_val_score(logreg, x_ET_10, y, scoring='accuracy', cv=50)

# PLOT
labels = ["All","AN","AN_5","An_10","RFE_5","RFE_10","ET_5","ET_10"]
fig, ax = plt.subplots()
rects1 = ax.boxplot([scores1,scores2,scores3,scores4,scores5,scores6,scores7])
ax.set_ylabel('Accuracy')
ax.set_title('Logistic regression')
ax.set_xticklabels(labels)
plt.show()

#Exmple
x_train,x_test,y_train,y_test = train_test_split(x_ET_10,y,test_size=0.2,random_state=1)
logreg = LogisticRegression(solver='liblinear').fit(x_train, y_train)

print("Training set score: {:.3f}".format(logreg.score(x_train, y_train)))
print("Test set score: {:.3f}".format(logreg.score(x_test, y_test)))

"""## KNN prediction





"""

from sklearn.datasets import load_iris 
from sklearn.model_selection import train_test_split 
from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier(n_neighbors = 5)
scores1 = cross_val_score(knn, x1, y, scoring='accuracy', cv=50)
scores2 = cross_val_score(knn, x_col_AN, y, scoring='accuracy', cv=50)
scores3 = cross_val_score(knn, x_col_AN_5, y, scoring='accuracy', cv=50)
scores4 = cross_val_score(knn, x_RFE_5, y, scoring='accuracy', cv=50)
scores5 = cross_val_score(knn, x_RFE_10, y, scoring='accuracy', cv=50)
scores6 = cross_val_score(knn, x_ET_5, y, scoring='accuracy', cv=50)
scores7 = cross_val_score(knn, x_ET_10, y, scoring='accuracy', cv=50)

# PLOT
labels = ["All","AN","AN_5","An_10","RFE_5","RFE_10","ET_5","ET_10"]
fig, ax = plt.subplots()
rects1 = ax.boxplot([scores1,scores2,scores3,scores4,scores5,scores6,scores7])
ax.set_ylabel('Accuracy')
ax.set_title('KNN')
ax.set_xticklabels(labels)
plt.show()

#Exmple
x_train,x_test,y_train,y_test = train_test_split(x_col_AN,y,test_size=0.2,random_state=1)
logreg = KNeighborsClassifier(n_neighbors = 5).fit(x_train, y_train)

print("Training set score: {:.3f}".format(logreg.score(x_train, y_train)))
print("Test set score: {:.3f}".format(logreg.score(x_test, y_test)))

"""## DLDA prediction"""

from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
DLDA = LinearDiscriminantAnalysis()
scores1 = cross_val_score(DLDA, x1, y, scoring='accuracy', cv=50)
scores2 = cross_val_score(DLDA, x_col_AN, y, scoring='accuracy', cv=50)
scores3 = cross_val_score(DLDA, x_col_AN_5, y, scoring='accuracy', cv=50)
scores4 = cross_val_score(DLDA, x_RFE_5, y, scoring='accuracy', cv=50)
scores5 = cross_val_score(DLDA, x_RFE_10, y, scoring='accuracy', cv=50)
scores6 = cross_val_score(DLDA, x_ET_5, y, scoring='accuracy', cv=50)
scores7 = cross_val_score(DLDA, x_ET_10, y, scoring='accuracy', cv=50)

# PLOT
labels = ["All","AN","AN_5","An_10","RFE_5","RFE_10","ET_5","ET_10"]
fig, ax = plt.subplots()
rects1 = ax.boxplot([scores1,scores2,scores3,scores4,scores5,scores6,scores7])
ax.set_ylabel('Accuracy')
ax.set_title('DLDA')
ax.set_xticklabels(labels)
plt.show()

#Exmple
x_train,x_test,y_train,y_test = train_test_split(x_col_AN,y,test_size=0.2,random_state=1)
logreg = LinearDiscriminantAnalysis().fit(x_train, y_train)

print("Training set score: {:.3f}".format(logreg.score(x_train, y_train)))
print("Test set score: {:.3f}".format(logreg.score(x_test, y_test)))

"""##Random Forest"""

from sklearn.ensemble import RandomForestClassifier
RDF = RandomForestClassifier(n_estimators=100)
scores1 = cross_val_score(RDF, x1, y, scoring='accuracy', cv=50)
scores2 = cross_val_score(RDF, x_col_AN, y, scoring='accuracy', cv=50)
scores3 = cross_val_score(RDF, x_col_AN_5, y, scoring='accuracy', cv=50)
scores4 = cross_val_score(RDF, x_RFE_5, y, scoring='accuracy', cv=50)
scores5 = cross_val_score(RDF, x_RFE_10, y, scoring='accuracy', cv=50)
scores6 = cross_val_score(RDF, x_ET_5, y, scoring='accuracy', cv=50)
scores7 = cross_val_score(RDF, x_ET_10, y, scoring='accuracy', cv=50)

# PLOT
labels = ["All","AN","AN_5","An_10","RFE_5","RFE_10","ET_5","ET_10"]
fig, ax = plt.subplots()
rects1 = ax.boxplot([scores1,scores2,scores3,scores4,scores5,scores6,scores7])
ax.set_ylabel('Accuracy')
ax.set_title('Random Forest')
ax.set_xticklabels(labels)
plt.show()

#Exmple
x_train,x_test,y_train,y_test = train_test_split(x_ET_5,y,test_size=0.2,random_state=1)
model = RandomForestClassifier(n_estimators=100).fit(x_train, y_train)

print("Training set score: {:.3f}".format(model.score(x_train, y_train)))
print("Test set score: {:.3f}".format(model.score(x_test, y_test)))

"""# Fuction rewrite"""

from scipy.io import wavfile
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import KFold



#########
# samplerate and Y can be change to streaming file path if neend
# modle can be use as any model
#########
def streaming_classifier(samplerate,Y,model):

    ## def streaming_classifier(path,model):
    ##    samplerate, Y = wavfile.read(path)

    xtime = np.array(range(0, len(Y)))/samplerate
    window_size = samplerate
    increment = int(window_size/3)
    thresh = 750


    predicted_labels = []  # stores predicted
    lower_interval = 0  # used to increment window
    max_time = int(max(xtime) * samplerate)

    predicted = []
    # initialing signal vector
    counter = 0
    is_event = []


    while (max_time > lower_interval + window_size):

        if max_time < lower_interval + window_size + increment:
            upper_interval = max_time
        else:
            upper_interval = lower_interval + window_size

        interval = Y[lower_interval:upper_interval]
        xinterval = xtime[lower_interval:upper_interval]  # gets corresponding time

        zerocrossing = (np.diff(np.sign(interval)) != 0).sum()
        Mean_value = np.mean(interval)
        standarddeviation = round(np.std(interval),3)
        abssum = sum(map(abs, interval))/10000
        #print(abssum,standarddeviation,counter,lower_interval,upper_interval)

        # If it is a event, recored it as True and add one to counter
        if abssum > thresh and upper_interval != max_time:
            is_event.append(True)
            counter = counter + 1
            lower_interval = lower_interval + increment

        # If ends, and the counter is greater than 0 which means it has event not finished
        elif upper_interval == max_time and counter > 0:
            begin_time = lower_interval - increment * counter
            end_time = max_time

            #ADD EVENT INTO LIST AND PRINT THE prediction
            current_value = Y[begin_time:end_time]
            # Predict by model
            y_pred=model.predict(current_value)
            predicted.append([begin_time,end_time,end_time-begin_time,Y[begin_time:end_time],y_pred])
            print(y_pred)
            ######################################
            # Moss code recognition application is added here
            ######################################
            lower_interval = lower_interval + increment

        # If it is not a event, back to its previous one and adjust whether its previous is event or not
        else:
            is_event.append(False)

            if len(is_event) == 1:
                lower_interval = lower_interval + increment
            elif is_event[-2] == True:
                begin_time = lower_interval - increment * counter
                end_time = lower_interval - increment + window_size
                
                #ADD EVENT INTO LIST AND PRINT THE prediction
                current_value = Y[begin_time:end_time]
                # Predict by model
                y_pred=model.predict(current_value)
                predicted.append([begin_time,end_time,end_time-begin_time,Y[begin_time:end_time],y_pred])
                print(y_pred)
                #########################################
                # Moss code recognition application is added here
                #########################################
                #print(begin_time,end_time,end_time-begin_time)
                lower_interval = end_time
            else:
                lower_interval = lower_interval + increment
            counter = 0

        
    df = pd.DataFrame(predicted,columns=['begin','end','Long','Values',"type"])
    return df